Tratamento de dados e limpeza de dados
2 minutes ago (5s)
42
42
# checa valoes null na coluna delay
df.filter("delay is NULL").show()
(3) Spark Jobs
 
+--------------------+-----+--------+------+-----------+
|                date|delay|distance|origin|destination|
+--------------------+-----+--------+------+-----------+
|Abbotsford\tBC\tC...| null|    null|  null|       null|
|Aberdeen\tSD\tUSA...| null|    null|  null|       null|
|Abilene\tTX\tUSA\...| null|    null|  null|       null|
| Akron\tOH\tUSA\tCAK| null|    null|  null|       null|
|Alamosa\tCO\tUSA\...| null|    null|  null|       null|
|Albany\tGA\tUSA\tABY| null|    null|  null|       null|
|Albany\tNY\tUSA\tALB| null|    null|  null|       null|
|Albuquerque\tNM\t...| null|    null|  null|       null|
|Alexandria\tLA\tU...| null|    null|  null|       null|
|Allentown\tPA\tUS...| null|    null|  null|       null|
|Alliance\tNE\tUSA...| null|    null|  null|       null|
|Alpena\tMI\tUSA\tAPN| null|    null|  null|       null|
|Altoona\tPA\tUSA\...| null|    null|  null|       null|
|Amarillo\tTX\tUSA...| null|    null|  null|       null|
|Anahim Lake\tBC\t...| null|    null|  null|       null|
|Anchorage\tAK\tUS...| null|    null|  null|       null|
|Appleton\tWI\tUSA...| null|    null|  null|       null|
|Arviat\tNWT\tCana...| null|    null|  null|       null|
|Asheville\tNC\tUS...| null|    null|  null|       null|
| Aspen\tCO\tUSA\tASE| null|    null|  null|       null|
+--------------------+-----+--------+------+-----------+
only showing top 20 rows

2 minutes ago (17s)
43
43
# conta a quantidade de linhas nulas
print ("Valores nulos coluna Delay: {0}".format(df.filter("delay is NULL").count()))
print ("Valores nulos coluna Date: {0}".format(df.filter("date is NULL").count()))
print ("Valores nulos coluna Distance: {0}".format(df.filter("distance is NULL").count()))
print ("Valores nulos coluna Origin: {0}".format(df.filter("origin is NULL").count()))
print ("Valores nulos coluna Destination: {0}".format(df.filter("destination is NULL").count()))
(10) Spark Jobs
 
Valores nulos coluna Delay: 528
Valores nulos coluna Date: 0
Valores nulos coluna Distance: 528
Valores nulos coluna Origin: 528
Valores nulos coluna Destination: 528
1 minute ago (<1s)
44
44
# preenche os dados missing com o valor 0
# para fazer o preenchimento sobrescreva a variável df e retire o método show()
df = df.na.fill(value=0)
df:pyspark.sql.dataframe.DataFrame
date:string
delay:integer
distance:integer
origin:string
destination:string
1 minute ago (<1s)
45
45
# checa valoes null na coluna delay
df.filter("delay is NULL").show()
+----+-----+--------+------+-----------+
|date|delay|distance|origin|destination|
+----+-----+--------+------+-----------+
+----+-----+--------+------+-----------+

1 minute ago (1s)
46
46
# preenche valores missing com valor 0 apenas da coluna delay
df.na.fill(value=0, subset=['delay']).show()
(1) Spark Jobs
 
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01011245|    6|     602|   ABE|        ATL|
|01020600|   -8|     369|   ABE|        DTW|
|01021245|   -2|     602|   ABE|        ATL|
|01020605|   -4|     602|   ABE|        ATL|
|01031245|   -4|     602|   ABE|        ATL|
|01030605|    0|     602|   ABE|        ATL|
|01041243|   10|     602|   ABE|        ATL|
|01040605|   28|     602|   ABE|        ATL|
|01051245|   88|     602|   ABE|        ATL|
|01050605|    9|     602|   ABE|        ATL|
|01061215|   -6|     602|   ABE|        ATL|
|01061725|   69|     602|   ABE|        ATL|
|01061230|    0|     369|   ABE|        DTW|
|01060625|   -3|     602|   ABE|        ATL|
|01070600|    0|     369|   ABE|        DTW|
|01071725|    0|     602|   ABE|        ATL|
|01071230|    0|     369|   ABE|        DTW|
|01070625|    0|     602|   ABE|        ATL|
|01071219|    0|     569|   ABE|        ORD|
|01080600|    0|     369|   ABE|        DTW|
+--------+-----+--------+------+-----------+
only showing top 20 rows

1 minute ago (1s)
47
47
# imprime o dataframe
df.show()
(1) Spark Jobs
 
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01011245|    6|     602|   ABE|        ATL|
|01020600|   -8|     369|   ABE|        DTW|
|01021245|   -2|     602|   ABE|        ATL|
|01020605|   -4|     602|   ABE|        ATL|
|01031245|   -4|     602|   ABE|        ATL|
|01030605|    0|     602|   ABE|        ATL|
|01041243|   10|     602|   ABE|        ATL|
|01040605|   28|     602|   ABE|        ATL|
|01051245|   88|     602|   ABE|        ATL|
|01050605|    9|     602|   ABE|        ATL|
|01061215|   -6|     602|   ABE|        ATL|
|01061725|   69|     602|   ABE|        ATL|
|01061230|    0|     369|   ABE|        DTW|
|01060625|   -3|     602|   ABE|        ATL|
|01070600|    0|     369|   ABE|        DTW|
|01071725|    0|     602|   ABE|        ATL|
|01071230|    0|     369|   ABE|        DTW|
|01070625|    0|     602|   ABE|        ATL|
|01071219|    0|     569|   ABE|        ORD|
|01080600|    0|     369|   ABE|        DTW|
+--------+-----+--------+------+-----------+
only showing top 20 rows

1 minute ago (1s)
48
48
# preenche os dados com valores de string vazia
df.na.fill("").show()
(1) Spark Jobs
 
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01011245|    6|     602|   ABE|        ATL|
|01020600|   -8|     369|   ABE|        DTW|
|01021245|   -2|     602|   ABE|        ATL|
|01020605|   -4|     602|   ABE|        ATL|
|01031245|   -4|     602|   ABE|        ATL|
|01030605|    0|     602|   ABE|        ATL|
|01041243|   10|     602|   ABE|        ATL|
|01040605|   28|     602|   ABE|        ATL|
|01051245|   88|     602|   ABE|        ATL|
|01050605|    9|     602|   ABE|        ATL|
|01061215|   -6|     602|   ABE|        ATL|
|01061725|   69|     602|   ABE|        ATL|
|01061230|    0|     369|   ABE|        DTW|
|01060625|   -3|     602|   ABE|        ATL|
|01070600|    0|     369|   ABE|        DTW|
|01071725|    0|     602|   ABE|        ATL|
|01071230|    0|     369|   ABE|        DTW|
|01070625|    0|     602|   ABE|        ATL|
|01071219|    0|     569|   ABE|        ORD|
|01080600|    0|     369|   ABE|        DTW|
+--------+-----+--------+------+-----------+
only showing top 20 rows

1 minute ago (<1s)
49
49
# remove qualquer linha nula de qualquer coluna
df = df.na.drop()
df:pyspark.sql.dataframe.DataFrame
date:string
delay:integer
distance:integer
origin:string
destination:string
just now (5s)
50
50
# obtem o valor máximo da coluna delay
from pyspark.sql.functions import max
df.select(max("delay")).take(1)
(2) Spark Jobs
 
Out[21]: [Row(max(delay)=1642)]
just now (1s)
51
51
# Filtrando linhas de um dataframe usando filter
df.filter("delay < 2").show(2)
(1) Spark Jobs
 
+--------+-----+--------+------+-----------+
|    date|delay|distance|origin|destination|
+--------+-----+--------+------+-----------+
|01020600|   -8|     369|   ABE|        DTW|
|01021245|   -2|     602|   ABE|        ATL|
+--------+-----+--------+------+-----------+
only showing top 2 rows
